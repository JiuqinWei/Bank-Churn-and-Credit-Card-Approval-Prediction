# Plot log(lambda) vs. deviance
plot(cv.out)
title(main = "Log(lambda) vs Deviance")
# Fit the final model with the best lambda
lasso_model <- glmnet(x_train, y_train, family = "multinomial", alpha = 1, lambda = best_lambda)
# Coefficients of the model
coef(lasso_model)
# Prepare test data for prediction
x_test <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = test_data)
# Predict class probabilities on the test set
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
# Convert predicted probabilities to predicted classes
lasso_pred_class <- apply(lasso_pred_prob, 1, which.max)
# Confusion matrix
confusionMatrix(as.factor(lasso_pred_class), as.factor(test_data$Avg_Status))
# For multi-class AUC, we can use the multi-class AUC function from the HandTill2001 package
install.packages("HandTill2001")
library(HandTill2001)
# One-hot encode the actual test labels
test_labels <- model.matrix(~ Avg_Status - 1, data = test_data)
# Compute multi-class AUC
auc_lasso <- multiClassSummary(data = data.frame(obs = test_data$Avg_Status, pred = lasso_pred_class, pred_prob = lasso_pred_prob), lev = levels(test_data$Avg_Status))
print(auc_lasso)
# Multinomial Logistic Regression with LASSO
# Load necessary libraries
library(caret)
library(glmnet)
library(pROC)
library(HandTill2001)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Prepare data for glmnet
x_train <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = train_data)
y_train <- as.factor(train_data$Avg_Status)
# Fit LASSO model
set.seed(123)
cv.out <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
best_lambda <- cv.out$lambda.min
# Plot log(lambda) vs. deviance
plot(cv.out)
title(main = "Log(lambda) vs Deviance")
# Plot log(lambda) vs. deviance
plot(cv.out, title= "Log(lambda) vs Deviance")
# Fit the final model with the best lambda
lasso_model <- glmnet(x_train, y_train, family = "multinomial", alpha = 1, lambda = best_lambda)
# Coefficients of the model
print(coef(lasso_model))
# Prepare test data for prediction
x_test <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = test_data)
# Predict class probabilities on the test set
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
# Convert predicted probabilities to predicted classes
lasso_pred_class <- apply(lasso_pred_prob, 1, which.max)
lasso_pred_class <- factor(lasso_pred_class, levels = 1:length(levels(test_data$Avg_Status)), labels = levels(test_data$Avg_Status))
# Confusion matrix
confusionMatrix(lasso_pred_class, test_data$Avg_Status)
# One-hot encode the actual test labels for multi-class AUC calculation
test_labels <- model.matrix(~ Avg_Status - 1, data = test_data)
# Compute multi-class AUC using HandTill2001
# Note: HandTill2001::auc works on one-hot encoded actual and predicted probabilities
auc_lasso <- HandTill2001::auc(multcap(test_labels, lasso_pred_prob))
print(auc_lasso)
library(caret)
library(glmnet)
library(pROC)
library(HandTill2001)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Prepare data for glmnet
x_train <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = train_data)
y_train <- as.factor(train_data$Avg_Status)
# Fit LASSO model
set.seed(123)
cv.out <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
best_lambda <- cv.out$lambda.min
# Plot log(lambda) vs. deviance
plot(cv.out)
title(main = "Log(lambda) vs Deviance")
# Fit the final model with the best lambda
lasso_model <- glmnet(x_train, y_train, family = "multinomial", alpha = 1, lambda = best_lambda)
# Coefficients of the model
print(coef(lasso_model))
# Prepare test data for prediction
x_test <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = test_data)
# Predict class probabilities on the test set
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
# Convert predicted probabilities to predicted classes
lasso_pred_class <- apply(lasso_pred_prob, 1, which.max)
lasso_pred_class <- factor(lasso_pred_class, levels = 1:length(levels(test_data$Avg_Status)), labels = levels(test_data$Avg_Status))
# Confusion matrix
confusionMatrix(lasso_pred_class, test_data$Avg_Status)
# Ensure the predicted probabilities are in a matrix format
lasso_pred_prob_matrix <- as.matrix(lasso_pred_prob)
# Ensure the response is a factor
test_labels <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Compute multi-class AUC using HandTill2001
auc_lasso <- HandTill2001::auc(multcap(response = test_labels, predicted = lasso_pred_prob_matrix))
print(auc_lasso)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Prepare data for glmnet
x_train <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = train_data)
y_train <- as.factor(train_data$Avg_Status)
# Fit LASSO model
set.seed(123)
cv.out <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
best_lambda <- cv.out$lambda.min
# Plot log(lambda) vs. deviance
plot(cv.out)
title(main = "Log(lambda) vs Deviance")
# Fit the final model with the best lambda
lasso_model <- glmnet(x_train, y_train, family = "multinomial", alpha = 1, lambda = best_lambda)
# Coefficients of the model
print(coef(lasso_model))
# Prepare test data for prediction
x_test <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = test_data)
# Predict class probabilities on the test set
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
# Convert predicted probabilities to predicted classes
lasso_pred_class <- apply(lasso_pred_prob, 1, which.max)
lasso_pred_class <- factor(lasso_pred_class, levels = 1:length(levels(test_data$Avg_Status)), labels = levels(test_data$Avg_Status))
# Confusion matrix
confusionMatrix(lasso_pred_class, test_data$Avg_Status)
# Ensure the predicted probabilities are in a matrix format and match the response levels
lasso_pred_prob_matrix <- as.matrix(lasso_pred_prob)
colnames(lasso_pred_prob_matrix) <- levels(test_data$Avg_Status)
# Ensure the response is a factor
test_labels <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Compute multi-class AUC using HandTill2001
auc_lasso <- HandTill2001::auc(multcap(response = test_labels, predicted = lasso_pred_prob_matrix))
print(auc_lasso)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Prepare data for glmnet
x_train <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = train_data)
y_train <- as.factor(train_data$Avg_Status)
# Fit LASSO model
set.seed(123)
cv.out <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
best_lambda <- cv.out$lambda.min
# Plot log(lambda) vs. deviance
plot(cv.out)
title(main = "Log(lambda) vs Deviance")
# Fit the final model with the best lambda
lasso_model <- glmnet(x_train, y_train, family = "multinomial", alpha = 1, lambda = best_lambda)
# Coefficients of the model
print(coef(lasso_model))
# Prepare test data for prediction
x_test <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = test_data)
# Predict class probabilities on the test set
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
# Ensure the predicted probabilities are in a matrix format and match the response levels
lasso_pred_prob_matrix <- as.matrix(lasso_pred_prob)
# Check if all levels are present in the prediction matrix
missing_levels <- setdiff(levels(test_data$Avg_Status), colnames(lasso_pred_prob_matrix))
if (length(missing_levels) > 0) {
# Add missing columns filled with NA
for (level in missing_levels) {
lasso_pred_prob_matrix <- cbind(lasso_pred_prob_matrix, NA)
colnames(lasso_pred_prob_matrix)[ncol(lasso_pred_prob_matrix)] <- level
}
}
# Reorder columns to match the levels of the response
lasso_pred_prob_matrix <- lasso_pred_prob_matrix[, levels(test_data$Avg_Status)]
# Convert predicted probabilities to predicted classes
lasso_pred_class <- apply(lasso_pred_prob_matrix, 1, which.max)
lasso_pred_class <- factor(lasso_pred_class, levels = 1:length(levels(test_data$Avg_Status)), labels = levels(test_data$Avg_Status))
# Confusion matrix
confusionMatrix(lasso_pred_class, test_data$Avg_Status)
# Ensure the response is a factor
test_labels <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Compute multi-class AUC using HandTill2001
auc_lasso <- HandTill2001::auc(multcap(response = test_labels, predicted = lasso_pred_prob_matrix))
print(auc_lasso)
library(caret)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Prepare data for glmnet
x_train <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = train_data)
y_train <- as.factor(train_data$Avg_Status)
# Fit LASSO model
set.seed(123)
cv.out <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
best_lambda <- cv.out$lambda.min
# Plot log(lambda) vs. deviance
plot(cv.out)
title(main = "Log(lambda) vs Deviance")
# Fit the final model with the best lambda
lasso_model <- glmnet(x_train, y_train, family = "multinomial", alpha = 1, lambda = best_lambda)
# Coefficients of the model
print(coef(lasso_model))
# Prepare test data for prediction
x_test <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = test_data)
# Predict class probabilities on the test set
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
# Ensure the predicted probabilities are in a matrix format and match the response levels
lasso_pred_prob_matrix <- as.matrix(lasso_pred_prob)
colnames(lasso_pred_prob_matrix) <- levels(test_data$Avg_Status)
# Check if all levels are present in the prediction matrix
missing_levels <- setdiff(levels(test_data$Avg_Status), colnames(lasso_pred_prob_matrix))
if (length(missing_levels) > 0) {
# Add missing columns filled with NA
for (level in missing_levels) {
lasso_pred_prob_matrix <- cbind(lasso_pred_prob_matrix, NA)
colnames(lasso_pred_prob_matrix)[ncol(lasso_pred_prob_matrix)] <- level
}
}
# Reorder columns to match the levels of the response
lasso_pred_prob_matrix <- lasso_pred_prob_matrix[, levels(test_data$Avg_Status)]
# Convert predicted probabilities to predicted classes
lasso_pred_class <- apply(lasso_pred_prob_matrix, 1, which.max)
lasso_pred_class <- factor(lasso_pred_class, levels = 1:length(levels(test_data$Avg_Status)), labels = levels(test_data$Avg_Status))
# Confusion matrix
print(length(lasso_pred_class))
print(length(test_data$Avg_Status))
confusionMatrix(lasso_pred_class, test_data$Avg_Status)
# Ensure the response is a factor
test_labels <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Compute multi-class AUC using HandTill2001
auc_lasso <- HandTill2001::auc(multcap(response = test_labels, predicted = lasso_pred_prob_matrix))
print(auc_lasso)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Prepare data for glmnet
x_train <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = train_data)
y_train <- as.factor(train_data$Avg_Status)
# Fit LASSO model
set.seed(123)
cv.out <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
best_lambda <- cv.out$lambda.min
# Plot log(lambda) vs. deviance
plot(cv.out)
title(main = "Log(lambda) vs Deviance")
# Fit the final model with the best lambda
lasso_model <- glmnet(x_train, y_train, family = "multinomial", alpha = 1, lambda = best_lambda)
# Coefficients of the model
print(coef(lasso_model))
# Prepare test data for prediction
x_test <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = test_data)
# Predict class probabilities on the test set
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
print(dim(lasso_pred_prob))  # Debug: Check dimensions
# Ensure the predicted probabilities are in a matrix format and match the response levels
lasso_pred_prob_matrix <- as.matrix(lasso_pred_prob)
# Check if all levels are present in the prediction matrix
missing_levels <- setdiff(levels(test_data$Avg_Status), colnames(lasso_pred_prob_matrix))
print(missing_levels)  # Debug: Check missing levels
if (length(missing_levels) > 0) {
# Add missing columns filled with NA
for (level in missing_levels) {
lasso_pred_prob_matrix <- cbind(lasso_pred_prob_matrix, NA)
colnames(lasso_pred_prob_matrix)[ncol(lasso_pred_prob_matrix)] <- level
}
}
# Reorder columns to match the levels of the response
lasso_pred_prob_matrix <- lasso_pred_prob_matrix[, levels(test_data$Avg_Status)]
print(dim(lasso_pred_prob_matrix))  # Debug: Check dimensions after reordering
# Convert predicted probabilities to predicted classes
lasso_pred_class <- apply(lasso_pred_prob_matrix, 1, which.max)
print(length(lasso_pred_class))  # Debug: Check length of predicted classes
lasso_pred_class <- factor(lasso_pred_class, levels = 1:length(levels(test_data$Avg_Status)), labels = levels(test_data$Avg_Status))
print(length(lasso_pred_class))  # Debug: Check length after factor conversion
# Confusion matrix
print(length(lasso_pred_class))
print(length(test_data$Avg_Status))
confusionMatrix(lasso_pred_class, test_data$Avg_Status)
# Ensure the response is a factor
test_labels <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Compute multi-class AUC using HandTill2001
auc_lasso <- HandTill2001::auc(multcap(response = test_labels, predicted = lasso_pred_prob_matrix))
print(auc_lasso)
library(caret)
library(glmnet)
library(pROC)
library(HandTill2001)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Prepare data for glmnet
x_train <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = train_data)
y_train <- as.factor(train_data$Avg_Status)
# Fit LASSO model
set.seed(123)
cv.out <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
best_lambda <- cv.out$lambda.min
# Plot log(lambda) vs. deviance
plot(cv.out)
title(main = "Log(lambda) vs Deviance")
# Fit the final model with the best lambda
lasso_model <- glmnet(x_train, y_train, family = "multinomial", alpha = 1, lambda = best_lambda)
# Coefficients of the model
print(coef(lasso_model))
# Prepare test data for prediction
x_test <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = test_data)
# Predict class probabilities on the test set
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
# Convert the three-dimensional array to a two-dimensional matrix
lasso_pred_prob_matrix <- matrix(aperm(lasso_pred_prob, c(1, 3, 2)), nrow = nrow(test_data), ncol = length(levels(test_data$Avg_Status)))
colnames(lasso_pred_prob_matrix) <- levels(test_data$Avg_Status)
# Check if all levels are present in the prediction matrix
missing_levels <- setdiff(levels(test_data$Avg_Status), colnames(lasso_pred_prob_matrix))
if (length(missing_levels) > 0) {
# Add missing columns filled with NA
for (level in missing_levels) {
lasso_pred_prob_matrix <- cbind(lasso_pred_prob_matrix, NA)
colnames(lasso_pred_prob_matrix)[ncol(lasso_pred_prob_matrix)] <- level
}
}
# Reorder columns to match the levels of the response
lasso_pred_prob_matrix <- lasso_pred_prob_matrix[, levels(test_data$Avg_Status)]
print(dim(lasso_pred_prob_matrix))  # Debug: Check dimensions after reordering
# Convert predicted probabilities to predicted classes
lasso_pred_class <- apply(lasso_pred_prob_matrix, 1, which.max)
print(length(lasso_pred_class))  # Debug: Check length of predicted classes
lasso_pred_class <- factor(lasso_pred_class, levels = 1:length(levels(test_data$Avg_Status)), labels = levels(test_data$Avg_Status))
print(length(lasso_pred_class))  # Debug: Check length after factor conversion
# Confusion matrix
print(length(lasso_pred_class))
print(length(test_data$Avg_Status))
confusionMatrix(lasso_pred_class, test_data$Avg_Status)
# Ensure the response is a factor
test_labels <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Compute multi-class AUC using HandTill2001
auc_lasso <- HandTill2001::auc(multcap(response = test_labels, predicted = lasso_pred_prob_matrix))
print(auc_lasso)
library(mgcv)
library(caret)
library(pROC)
library(HandTill2001)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Fit the GAM model
gam_model <- gam(Avg_Status ~ s(Age) + s(Inc) + s(Inctp) + s(Edutp) + s(Houtp) + Wkphone + Phone,
family = multinom(K = length(levels(train_data$Avg_Status))),
data = train_data)
library(mgcv)
library(caret)
library(pROC)
library(HandTill2001)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Fit the GAM model with adjusted smoothing terms
gam_model <- gam(Avg_Status ~ s(Age, k = 5) + s(Inc, k = 5) + s(Inctp, k = 5) + s(Edutp, k = 5) + s(Houtp, k = 5) + Wkphone + Phone,
family = multinom(K = length(levels(train_data$Avg_Status))),
data = train_data)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Check for missing values
sum(is.na(train_data))
sum(is.na(test_data))
# Fit a simple GAM model
gam_model_simple <- gam(Avg_Status ~ s(Age, k = 5),
family = multinom(K = length(levels(train_data$Avg_Status))),
data = train_data)
# If the simple model works, gradually add more terms
gam_model <- gam(Avg_Status ~ s(Age, k = 5) + s(Inc, k = 5) + s(Inctp, k = 5) + s(Edutp, k = 5) + s(Houtp, k = 5) + Wkphone + Phone,
family = multinom(K = length(levels(train_data$Avg_Status))),
data = train_data)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Base logistic regression model
m0 <- multinom(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone, data = train_data, trace = FALSE)
# Multinomial Logistic Regression with LASSO
# Load necessary libraries
install.packages("caret")
install.packages("glmnet")
install.packages("pROC")
install.packages("HandTill2001")
library(caret)
library(glmnet)
library(pROC)
library(HandTill2001)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Prepare data for glmnet
x_train <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = train_data)
y_train <- as.factor(train_data$Avg_Status)
# Fit LASSO model
set.seed(123)
cv.out <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
best_lambda <- cv.out$lambda.min
# Plot log(lambda) vs. deviance
plot(cv.out)
title(main = "Log(lambda) vs Deviance")
# Fit the final model with the best lambda
lasso_model <- glmnet(x_train, y_train, family = "multinomial", alpha = 1, lambda = best_lambda)
# Coefficients of the model
print(coef(lasso_model))
# Prepare test data for prediction
x_test <- model.matrix(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone - 1, data = test_data)
# Predict class probabilities on the test set
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
# Convert the three-dimensional array to a two-dimensional matrix
lasso_pred_prob_matrix <- matrix(aperm(lasso_pred_prob, c(1, 3, 2)), nrow = nrow(test_data), ncol = length(levels(test_data$Avg_Status)))
print(auc_lasso)
library(mgcv)
library(nnet)
install.packages("nnet")
library(mgcv)
library(nnet)
library(caret)
library(pROC)
library(HandTill2001)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Base logistic regression model
m0 <- multinom(Avg_Status ~ Age + Inc + Inctp + Edutp + Houtp + Wkphone + Phone, data = train_data)
summary(m0)
# Predict on the test set using the base model
logit_pred_prob <- predict(m0, newdata = test_data, type = "probs")
logit_pred_class <- predict(m0, newdata = test_data)
# Confusion matrix for base model
confusionMatrix(logit_pred_class, test_data$Avg_Status)
# GAM model with separate smoothing terms
m1 <- gam(Avg_Status ~ s(Age) + s(Inc) + s(Inctp) + s(Edutp) + s(Houtp) + Wkphone + Phone,
family = multinom(K = length(levels(train_data$Avg_Status))),
data = train_data)
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(result_balanced$Avg_Status, p = 0.7, list = FALSE, times = 1)
train_data <- result_balanced[trainIndex,]
test_data <- result_balanced[-trainIndex,]
# Ensure Avg_Status is a factor and levels match between train and test
train_data$Avg_Status <- factor(train_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
test_data$Avg_Status <- factor(test_data$Avg_Status, levels = levels(result_balanced$Avg_Status))
# Define the response levels
response_levels <- levels(train_data$Avg_Status)
# Fit GAM models for each class vs. the rest (one-vs-rest approach)
gam_models <- lapply(response_levels, function(level) {
binary_response <- ifelse(train_data$Avg_Status == level, 1, 0)
gam(binary_response ~ s(Age) + s(Inc) + s(Inctp) + s(Edutp) + s(Houtp) + Wkphone + Phone, family = binomial, data = train_data)
})
